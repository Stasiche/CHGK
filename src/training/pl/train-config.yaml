tokenizer: "sberbank-ai/rugpt3small_based_on_gpt2"
model: "sberbank-ai/rugpt3small_based_on_gpt2"
seed: 65537

dataset:
  path: "data/dataset_reduced_filtered_super_mega.csv"
  train_size: 0.9
  train_batch_size: 14
  val_batch_size: 32
  limit_batches: 1.0
  seq_len: 100
  ans_seq_len: 20


training:
  n_gpus: 0
  output_dir: "training_output/"
  save_model_dir: "training_output/model/"

  opt:
    lr: 1e-4
    w_decay: 0.00
    eps: 1e-8
    max_epochs: 2
    warmup_steps: 0.01
    grad_accumulation_steps: 1
    grad_clip: 0
    freeze: False
    scheduler: "cosine"
    swa: False
    early_stopping: False

  logging:
    val_check_interval: 0.1
    save_top_k: 2
    log_steps: 100

hydra:
  run:
    dir: training_output_data/${now:%Y-%m-%d}/${now:%H-%M-%S}
output:
  logger_dir: training_output_data/